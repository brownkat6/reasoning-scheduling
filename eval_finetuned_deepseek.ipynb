{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "085a7071",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In[ ]:\n",
    "\n",
    "import os, glob, pandas as pd, torch\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from transformers.modeling_outputs import SequenceClassifierOutput\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "import torch.nn as nn\n",
    "from safetensors.torch import load_file as load_safetensors\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# ── USER SETTINGS ──────────────────────────────────────────────────────────────\n",
    "output_dir = \"/n/netscratch/gershman_lab/Lab/amuppidi/reasoning\"\n",
    "model_name = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\"\n",
    "csv_dir    = \"/n/home04/amuppidi/reasoning-scheduling/data/gsm8k_results_with_difficulty\"\n",
    "use_lora   = True\n",
    "lora_r     = 16\n",
    "lora_alpha = 32\n",
    "batch_size = 8\n",
    "device     = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# ── HELPERS ────────────────────────────────────────────────────────────────────\n",
    "def load_difficulty_csvs(csv_dir, split):\n",
    "    pattern = os.path.join(csv_dir, f\"gsm8k_Y_{split}_*_with_difficulty.csv\")\n",
    "    paths   = sorted(glob.glob(pattern))\n",
    "    if not paths:\n",
    "        raise FileNotFoundError(pattern)\n",
    "    return pd.concat([\n",
    "        pd.read_csv(p, usecols=[\"question_text\",\"difficulty\"])\n",
    "        for p in paths\n",
    "    ], ignore_index=True)\n",
    "\n",
    "class DifficultyDataset(torch.utils.data.Dataset):\n",
    "    label2id = {\"easy\":0,\"medium\":1,\"hard\":2}\n",
    "    def __init__(self, df, tokenizer, max_length=512):\n",
    "        self.texts = df[\"question_text\"].tolist()\n",
    "        self.labels= [self.label2id[d] for d in df[\"difficulty\"]]\n",
    "        self.tokenizer, self.max_length = tokenizer, max_length\n",
    "    def __len__(self): return len(self.texts)\n",
    "    def __getitem__(self, i):\n",
    "        enc = self.tokenizer(\n",
    "            self.texts[i],\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=self.max_length,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        item = {k:v.squeeze(0) for k,v in enc.items()}\n",
    "        item[\"labels\"] = torch.tensor(self.labels[i],dtype=torch.long)\n",
    "        return item\n",
    "\n",
    "def collate_fn(batch):\n",
    "    return {\n",
    "        \"input_ids\":      torch.stack([b[\"input_ids\"]      for b in batch]),\n",
    "        \"attention_mask\": torch.stack([b[\"attention_mask\"] for b in batch]),\n",
    "        \"labels\":         torch.tensor([b[\"labels\"]         for b in batch])\n",
    "    }\n",
    "\n",
    "class DifficultyFinetuner(nn.Module):\n",
    "    def __init__(self, model_name, use_lora, r, alpha, num_labels):\n",
    "        super().__init__()\n",
    "        self.lm = AutoModelForCausalLM.from_pretrained(\n",
    "            model_name, output_hidden_states=True, device_map=\"auto\"\n",
    "        )\n",
    "        if use_lora:\n",
    "            cfg = LoraConfig(\n",
    "                task_type=TaskType.CAUSAL_LM,\n",
    "                inference_mode=False,\n",
    "                r=r, lora_alpha=alpha,\n",
    "                lora_dropout=0.05,\n",
    "                target_modules=[\"q_proj\",\"v_proj\"],\n",
    "            )\n",
    "            self.lm = get_peft_model(self.lm, cfg)\n",
    "        H = self.lm.config.hidden_size\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.LayerNorm(H),\n",
    "            nn.Linear(H, H//2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(H//2, num_labels),\n",
    "        )\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, labels=None):\n",
    "        out = self.lm(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            output_hidden_states=True,\n",
    "            return_dict=True\n",
    "        )\n",
    "        hs = out.hidden_states[-1]                    # (B, L, H)\n",
    "        lengths = attention_mask.sum(dim=1) - 1       # last real index\n",
    "        last   = hs[torch.arange(len(lengths)), lengths]\n",
    "        logits = self.classifier(last)\n",
    "        loss   = F.cross_entropy(logits, labels) if labels is not None else None\n",
    "        return SequenceClassifierOutput(loss=loss, logits=logits)\n",
    "\n",
    "from tqdm.notebook import tqdm  # Add this import\n",
    "\n",
    "def run_eval(model, loader, device):\n",
    "    model.eval()\n",
    "    preds, labs = [], []\n",
    "    with torch.no_grad():\n",
    "        for b in tqdm(loader, desc=\"Evaluating\", leave=False):  # Wrap the loader with tqdm\n",
    "            b_input = {k: v.to(device) for k, v in b.items()}\n",
    "            out = model(**{k: b_input[k] for k in (\"input_ids\", \"attention_mask\")})\n",
    "            p = out.logits.argmax(-1).cpu()\n",
    "            preds.append(p)\n",
    "            labs.append(b_input[\"labels\"].cpu())\n",
    "    preds = torch.cat(preds).numpy()\n",
    "    labs = torch.cat(labs).numpy()\n",
    "    return accuracy_score(labs, preds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "740238f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "563a8eeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/n/home04/amuppidi/.conda/envs/torch/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:820: UserWarning: `return_dict_in_generate` is NOT set to `True`, but `output_hidden_states` is. When `return_dict_in_generate` is not `True`, `output_hidden_states` is ignored.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ── LOAD TOKENIZER & MODEL ────────────────────────────────────────────────────\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
    "\n",
    "model = DifficultyFinetuner(\n",
    "    model_name, use_lora, lora_r, lora_alpha, num_labels=3\n",
    ").to(device)\n",
    "\n",
    "sd = load_safetensors(os.path.join(output_dir, \"model.safetensors\"), device=\"cuda:0\")\n",
    "model.load_state_dict(sd)\n",
    "\n",
    "# ── PREPARE DATALOADERS ───────────────────────────────────────────────────────\n",
    "train_df = load_difficulty_csvs(csv_dir, \"train\")\n",
    "test_df  = load_difficulty_csvs(csv_dir, \"test\")\n",
    "\n",
    "train_ds = DifficultyDataset(train_df, tokenizer)\n",
    "test_ds  = DifficultyDataset(test_df, tokenizer)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=batch_size, collate_fn=collate_fn)\n",
    "test_loader  = DataLoader(test_ds,  batch_size=batch_size, collate_fn=collate_fn)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "071a2fee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53cb261c3b824e0abfcf803b89acbfde",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/932 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97c36902d2e2496a94ac0e998f1e2075",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/162 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "▶ Train Accuracy = 74.44%\n",
      "▶ Test  Accuracy = 66.31%\n"
     ]
    }
   ],
   "source": [
    "# ── RUN & PRINT ───────────────────────────────────────────────────────────────\n",
    "train_acc = run_eval(model, train_loader, device)\n",
    "test_acc  = run_eval(model, test_loader,  device)\n",
    "print(f\"▶ Train Accuracy = {train_acc*100:.2f}%\")\n",
    "print(f\"▶ Test  Accuracy = {test_acc*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "260b9bd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/n/home04/amuppidi/.conda/envs/torch/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:820: UserWarning: `return_dict_in_generate` is NOT set to `True`, but `output_hidden_states` is. When `return_dict_in_generate` is not `True`, `output_hidden_states` is ignored.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Updated gsm8k_Y_train_0_with_difficulty.csv with 100 rows\n",
      "✅ Updated gsm8k_Y_train_10_with_difficulty.csv with 100 rows\n",
      "✅ Updated gsm8k_Y_train_11_with_difficulty.csv with 100 rows\n",
      "✅ Updated gsm8k_Y_train_12_with_difficulty.csv with 100 rows\n",
      "✅ Updated gsm8k_Y_train_13_with_difficulty.csv with 100 rows\n",
      "✅ Updated gsm8k_Y_train_14_with_difficulty.csv with 100 rows\n",
      "✅ Updated gsm8k_Y_train_15_with_difficulty.csv with 100 rows\n",
      "✅ Updated gsm8k_Y_train_16_with_difficulty.csv with 100 rows\n",
      "✅ Updated gsm8k_Y_train_17_with_difficulty.csv with 100 rows\n",
      "✅ Updated gsm8k_Y_train_18_with_difficulty.csv with 100 rows\n",
      "✅ Updated gsm8k_Y_train_19_with_difficulty.csv with 100 rows\n",
      "✅ Updated gsm8k_Y_train_1_with_difficulty.csv with 100 rows\n",
      "✅ Updated gsm8k_Y_train_20_with_difficulty.csv with 100 rows\n",
      "✅ Updated gsm8k_Y_train_21_with_difficulty.csv with 100 rows\n",
      "✅ Updated gsm8k_Y_train_22_with_difficulty.csv with 100 rows\n",
      "✅ Updated gsm8k_Y_train_23_with_difficulty.csv with 100 rows\n",
      "✅ Updated gsm8k_Y_train_24_with_difficulty.csv with 100 rows\n",
      "✅ Updated gsm8k_Y_train_25_with_difficulty.csv with 100 rows\n",
      "✅ Updated gsm8k_Y_train_26_with_difficulty.csv with 100 rows\n",
      "✅ Updated gsm8k_Y_train_27_with_difficulty.csv with 100 rows\n",
      "✅ Updated gsm8k_Y_train_28_with_difficulty.csv with 100 rows\n",
      "✅ Updated gsm8k_Y_train_29_with_difficulty.csv with 100 rows\n",
      "✅ Updated gsm8k_Y_train_2_with_difficulty.csv with 100 rows\n",
      "✅ Updated gsm8k_Y_train_30_with_difficulty.csv with 100 rows\n",
      "✅ Updated gsm8k_Y_train_31_with_difficulty.csv with 100 rows\n",
      "✅ Updated gsm8k_Y_train_32_with_difficulty.csv with 100 rows\n",
      "✅ Updated gsm8k_Y_train_33_with_difficulty.csv with 100 rows\n",
      "✅ Updated gsm8k_Y_train_34_with_difficulty.csv with 100 rows\n",
      "✅ Updated gsm8k_Y_train_35_with_difficulty.csv with 100 rows\n",
      "✅ Updated gsm8k_Y_train_36_with_difficulty.csv with 100 rows\n",
      "✅ Updated gsm8k_Y_train_37_with_difficulty.csv with 100 rows\n",
      "✅ Updated gsm8k_Y_train_38_with_difficulty.csv with 100 rows\n",
      "✅ Updated gsm8k_Y_train_39_with_difficulty.csv with 100 rows\n",
      "✅ Updated gsm8k_Y_train_3_with_difficulty.csv with 100 rows\n",
      "✅ Updated gsm8k_Y_train_40_with_difficulty.csv with 100 rows\n",
      "✅ Updated gsm8k_Y_train_41_with_difficulty.csv with 100 rows\n",
      "✅ Updated gsm8k_Y_train_42_with_difficulty.csv with 77 rows\n",
      "✅ Updated gsm8k_Y_train_43_with_difficulty.csv with 100 rows\n",
      "✅ Updated gsm8k_Y_train_44_with_difficulty.csv with 100 rows\n",
      "✅ Updated gsm8k_Y_train_45_with_difficulty.csv with 100 rows\n",
      "✅ Updated gsm8k_Y_train_46_with_difficulty.csv with 100 rows\n",
      "✅ Updated gsm8k_Y_train_47_with_difficulty.csv with 100 rows\n",
      "✅ Updated gsm8k_Y_train_48_with_difficulty.csv with 100 rows\n",
      "✅ Updated gsm8k_Y_train_49_with_difficulty.csv with 100 rows\n",
      "✅ Updated gsm8k_Y_train_4_with_difficulty.csv with 100 rows\n",
      "✅ Updated gsm8k_Y_train_50_with_difficulty.csv with 100 rows\n",
      "✅ Updated gsm8k_Y_train_51_with_difficulty.csv with 100 rows\n",
      "✅ Updated gsm8k_Y_train_52_with_difficulty.csv with 100 rows\n",
      "✅ Updated gsm8k_Y_train_53_with_difficulty.csv with 100 rows\n",
      "✅ Updated gsm8k_Y_train_54_with_difficulty.csv with 100 rows\n",
      "✅ Updated gsm8k_Y_train_55_with_difficulty.csv with 100 rows\n",
      "✅ Updated gsm8k_Y_train_56_with_difficulty.csv with 100 rows\n",
      "✅ Updated gsm8k_Y_train_57_with_difficulty.csv with 100 rows\n",
      "✅ Updated gsm8k_Y_train_58_with_difficulty.csv with 100 rows\n",
      "✅ Updated gsm8k_Y_train_59_with_difficulty.csv with 100 rows\n",
      "✅ Updated gsm8k_Y_train_5_with_difficulty.csv with 100 rows\n",
      "✅ Updated gsm8k_Y_train_60_with_difficulty.csv with 100 rows\n",
      "✅ Updated gsm8k_Y_train_61_with_difficulty.csv with 100 rows\n",
      "✅ Updated gsm8k_Y_train_62_with_difficulty.csv with 100 rows\n",
      "✅ Updated gsm8k_Y_train_63_with_difficulty.csv with 100 rows\n",
      "✅ Updated gsm8k_Y_train_64_with_difficulty.csv with 100 rows\n",
      "✅ Updated gsm8k_Y_train_65_with_difficulty.csv with 100 rows\n",
      "✅ Updated gsm8k_Y_train_66_with_difficulty.csv with 100 rows\n",
      "✅ Updated gsm8k_Y_train_67_with_difficulty.csv with 100 rows\n",
      "✅ Updated gsm8k_Y_train_68_with_difficulty.csv with 100 rows\n",
      "✅ Updated gsm8k_Y_train_69_with_difficulty.csv with 100 rows\n",
      "✅ Updated gsm8k_Y_train_6_with_difficulty.csv with 100 rows\n",
      "✅ Updated gsm8k_Y_train_70_with_difficulty.csv with 100 rows\n",
      "✅ Updated gsm8k_Y_train_71_with_difficulty.csv with 100 rows\n",
      "✅ Updated gsm8k_Y_train_72_with_difficulty.csv with 100 rows\n",
      "✅ Updated gsm8k_Y_train_73_with_difficulty.csv with 100 rows\n",
      "✅ Updated gsm8k_Y_train_74_with_difficulty.csv with 73 rows\n",
      "✅ Updated gsm8k_Y_train_7_with_difficulty.csv with 100 rows\n",
      "✅ Updated gsm8k_Y_train_8_with_difficulty.csv with 100 rows\n",
      "✅ Updated gsm8k_Y_train_9_with_difficulty.csv with 100 rows\n",
      "✅ Updated gsm8k_Y_test_0_with_difficulty.csv with 100 rows\n",
      "✅ Updated gsm8k_Y_test_10_with_difficulty.csv with 100 rows\n",
      "✅ Updated gsm8k_Y_test_11_with_difficulty.csv with 100 rows\n",
      "✅ Updated gsm8k_Y_test_12_with_difficulty.csv with 100 rows\n",
      "✅ Updated gsm8k_Y_test_13_with_difficulty.csv with 19 rows\n",
      "✅ Updated gsm8k_Y_test_1_with_difficulty.csv with 100 rows\n",
      "✅ Updated gsm8k_Y_test_2_with_difficulty.csv with 100 rows\n",
      "✅ Updated gsm8k_Y_test_3_with_difficulty.csv with 100 rows\n",
      "✅ Updated gsm8k_Y_test_4_with_difficulty.csv with 75 rows\n",
      "✅ Updated gsm8k_Y_test_5_with_difficulty.csv with 100 rows\n",
      "✅ Updated gsm8k_Y_test_6_with_difficulty.csv with 100 rows\n",
      "✅ Updated gsm8k_Y_test_7_with_difficulty.csv with 100 rows\n",
      "✅ Updated gsm8k_Y_test_8_with_difficulty.csv with 100 rows\n",
      "✅ Updated gsm8k_Y_test_9_with_difficulty.csv with 100 rows\n"
     ]
    }
   ],
   "source": [
    "# ── SETTINGS ───────────────────────────────────────────────────────────────────\n",
    "output_dir = \"/n/netscratch/gershman_lab/Lab/amuppidi/reasoning\"\n",
    "csv_base   = \"/n/home04/amuppidi/reasoning-scheduling/data/gsm8k_results_with_difficulty\"\n",
    "model_name = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\"\n",
    "use_lora   = True\n",
    "lora_r     = 16\n",
    "lora_alpha = 32\n",
    "batch_size = 16\n",
    "device     = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# ── LOAD TOKENIZER & MODEL ────────────────────────────────────────────────────\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
    "\n",
    "model = DifficultyFinetuner(\n",
    "    model_name=model_name,\n",
    "    use_lora=use_lora,\n",
    "    r=lora_r,\n",
    "    alpha=lora_alpha,\n",
    "    num_labels=3\n",
    ").to(device)\n",
    "\n",
    "# load the finetuned safetensors\n",
    "from safetensors.torch import load_file as load_safetensors\n",
    "sd = load_safetensors(os.path.join(output_dir, \"model.safetensors\"), device=\"cpu\")\n",
    "model.load_state_dict(sd)\n",
    "model.to(device)\n",
    "\n",
    "\n",
    "# ── PREDICTION UTIL ────────────────────────────────────────────────────────────\n",
    "def get_preds(model, loader):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            inp = {k: batch[k].to(device) for k in (\"input_ids\",\"attention_mask\")}\n",
    "            logits = model(**inp).logits\n",
    "            preds = logits.argmax(-1).cpu().numpy().tolist()\n",
    "            all_preds.extend(preds)\n",
    "    return all_preds\n",
    "\n",
    "# invert label map\n",
    "id2label = {v: k for k, v in DifficultyDataset.label2id.items()}\n",
    "\n",
    "\n",
    "# ── PROCESS EACH SPLIT & FILE ─────────────────────────────────────────────────\n",
    "import glob\n",
    "\n",
    "for split in [\"train\", \"test\"]:\n",
    "    pattern = os.path.join(csv_base, f\"gsm8k_Y_{split}_*_with_difficulty.csv\")\n",
    "    for path in sorted(glob.glob(pattern)):\n",
    "        # 1) load existing CSV\n",
    "        df = pd.read_csv(path)\n",
    "        # 2) build dataset & loader\n",
    "        ds     = DifficultyDataset(df, tokenizer)\n",
    "        loader = DataLoader(ds, batch_size=batch_size, collate_fn=collate_fn)\n",
    "        # 3) get raw preds (0/1/2)\n",
    "        raw_preds = get_preds(model, loader)\n",
    "        # 4) map to strings\n",
    "        df[\"model_predicted_difficulty\"] = [id2label[p] for p in raw_preds]\n",
    "        # 5) overwrite CSV\n",
    "        df.to_csv(path, index=False)\n",
    "        print(f\"✅ Updated {os.path.basename(path)} with {len(df)} rows\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6031fc7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
